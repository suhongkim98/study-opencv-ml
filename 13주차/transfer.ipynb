{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Flatten, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from  keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy  import expand_dims\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 개고양이 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "image_size = 224\n",
    "\n",
    "# 학습 이미지에 적용한 augmentation 인자를 지정해줍니다.\n",
    "# 너무 많은 노이즈를 주면 인식률에 영향을 줌\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 이미지를 배치 단위로 불러와 줄 generator입니다.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/smallcatdog/train',  # this is the target directory\n",
    "        target_size=(224, 224),  # 모든 이미지의 크기가 150x150로 조정됩니다.\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # binary_crossentropy 손실 함수를 사용하므로 binary 형태로 라벨을 불러와야 합니다.\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'dataset/smallcatdog/test',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# 두가지 클래스로 2000장, 800장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1)전이학습 : Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<keras.engine.input_layer.InputLayer object at 0x7fcc70b41550> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70b41760> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70b41610> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcccbfcb850> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70b420d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70b42df0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc70b42dc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70b56430> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70add1f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70adda90> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc70addfa0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70ae5340> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70ae5220> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70aeb130> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc70ae5760> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70af49d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70af4640> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70aee7c0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc70af4fd0> False\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 40,406,849\n",
      "Trainable params: 25,692,161\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN은 특징 추출과 분류기로 이루어짐\n",
    "#include_top을 false로 주면 특징 추출기까지만 구성된 weight를 가지게 된다.\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "vgg_conv.summary()\n",
    "\n",
    "for layer in vgg_conv.layers[:]:\n",
    "    layer.trainable = False # vgg 신경을 freeze해서 학습이 이루어지지 않도록 한다, 학습된 weight 사용\n",
    "\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 특징 추출을 VGG16으로 하겠다. 앞단을 vgg_conv\n",
    "model.add(vgg_conv) # vgg신경망 구조를 사용하곘다\n",
    "\n",
    "model.add(Flatten()) # 1차원으로 바꾸고\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 151s 1s/step - loss: 0.9035 - accuracy: 0.7995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc710e07f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 85.87%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator)\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)전이학습:Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7fcc7088e940> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc7088eb50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc708fb1f0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc7088ea00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc7089b7c0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70825940> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc7088ef10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc7081dbe0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc7089bd90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70836880> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc7081dc40> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc708363d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70840970> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70844c70> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc7084bf40> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc7083ab80> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70854880> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fcc70850c40> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fcc7084bbb0> True\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 40,406,849\n",
      "Trainable params: 32,771,585\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 전체적인 컨셉은 프리징과 비슷하다\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# 여기가 프리징과 다르다. 프리징은 사전학습모델 모든 레이어를 학습이 불가능하도록 false로 바꿨는데\n",
    "# fine tunning은 마지막 층 4개를 제외하고 false로 만들어 마지막 4개는 재학습을 하도록 한다.\n",
    "# 낮은 층에서 상위로 갈 수록 우리가 풀고자 하는 도메인에 비슷한 필터들이 학습\n",
    "# 낮은 층은 우리가 풀고자하는 도메인과 관련성이 떨어지므로 false를 주어 학습을 하지 않지만\n",
    "# 마지막 층은 우리가 풀고자 하는 도메인과 관련이 크기 때문에 재학습을 하도록 한다.\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 183s 1s/step - loss: 0.8741 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc7090fc40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator)\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 음식분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9866 images belonging to 11 classes.\n",
      "Found 3347 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# 학습 이미지에 적용한 augmentation 인자를 지정해줍니다.\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/food-11/training',  #11개의 음식을 구분하도록 한 데이터셋들\n",
    "        target_size=(224, 224), \n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'dataset/food-11/evaluation',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f4be067b8e0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4cd0792dc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0c21190> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4cd0792220> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0c3b760> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0c75a90> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4be0c48670> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0c957f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0c33d90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0cad370> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4be0c335e0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0beae20> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0beabb0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0bee340> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4be0bead00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0bf2d90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0bf0970> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4be0bf6e20> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4be0bf2d30> False\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 11)                11275     \n",
      "=================================================================\n",
      "Total params: 15,251,275\n",
      "Trainable params: 536,587\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "vgg_conv.summary()\n",
    "\n",
    "for layer in vgg_conv.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(vgg_conv)\n",
    "\n",
    "model.add(GlobalAveragePooling2D())                       \n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssongkim/projects/study-opencv-ml/.venv/lib/python3.8/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "2021-12-05 18:34:25.525769: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "617/617 [==============================] - 704s 1s/step - loss: 1.3962 - accuracy: 0.5245\n",
      "Epoch 2/10\n",
      "617/617 [==============================] - 701s 1s/step - loss: 1.0838 - accuracy: 0.6314\n",
      "Epoch 3/10\n",
      "617/617 [==============================] - 706s 1s/step - loss: 1.0035 - accuracy: 0.6546\n",
      "Epoch 4/10\n",
      "617/617 [==============================] - 704s 1s/step - loss: 0.9362 - accuracy: 0.6805\n",
      "Epoch 5/10\n",
      "617/617 [==============================] - 707s 1s/step - loss: 0.9081 - accuracy: 0.6893\n",
      "Epoch 6/10\n",
      "617/617 [==============================] - 700s 1s/step - loss: 0.8703 - accuracy: 0.6961\n",
      "Epoch 7/10\n",
      "617/617 [==============================] - 700s 1s/step - loss: 0.8430 - accuracy: 0.7119\n",
      "Epoch 8/10\n",
      "617/617 [==============================] - 701s 1s/step - loss: 0.8098 - accuracy: 0.7203\n",
      "Epoch 9/10\n",
      "617/617 [==============================] - 707s 1s/step - loss: 0.7771 - accuracy: 0.7341\n",
      "Epoch 10/10\n",
      "617/617 [==============================] - 703s 1s/step - loss: 0.7597 - accuracy: 0.7375\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssongkim/projects/study-opencv-ml/.venv/lib/python3.8/site-packages/keras/engine/training.py:2006: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 72.72%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator)\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4b7aa9cb20>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
